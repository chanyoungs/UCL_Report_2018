\chapter{Introduction} \label{ch:introduction}

    \section{Motivation}
        The success of machine learning algorithms in general depend on successful representation learning from data. From tasks such as hand writing recognition\citep{lecun1998gradient} to beating Atari games\citep{mnih2013playing}, a key aspect of these learning algorithm is to reduce the high dimensional input data into more meaningful lower dimension representation for further inferences. Many recent applications of deep learning methods have been successful(\cite{he2016deep}, \cite{szegedy2015going}, \cite{gregor2015draw}, \cite{van2016wavenet}, \cite{van2016conditional}, \cite{mnih2015human}, \cite{jaderberg2016reinforcement}, \cite{silver2016mastering}). However, as \cite{burgess2018understanding} points out, the representations learnt by these algorithms based on supervised or reward based reinforcement learning are very streamlined to be useful in solving their specific tasks but are not robust enough for generalising and domain transfer compared to the biological intelligence\citep{lake2017building}. This can be seen as a form of overfitting to the given task.
        
        To improve generalising the learnt representations, many approaches have been attempted including auxiliary tasks\citep{jaderberg2016reinforcement} and data augmentation\citep{tobin2017domain}, but a more recent promising approach is arguably in the field of \textit{disentangled representation}, the property of having a good one-to-one correspondence between the individual latent dimensions and the generative factors from the input space. One of the first unsupervised methods which showed this property is the Variational Autoencoder(VAE) by \cite{kingma2013auto}. There have been other notable methods since then such as the Generative Adversarial Network(GAN) based InfoGAN\citep{chen2016infogan}, but in this paper, we will focus on the VAE based method $\beta$-Variational Autoencoder($\beta$-VAE) by \cite{higgins2017beta} which has been shown to outperform the original VAE in disentanglement on various data sets. The interesting thing about the $\beta$-VAE is that there is only one difference from the original VAE and that is an addition of a single scalar hyperparameter $\beta$. In this paper, we wish to further test the disentanglement performance between the $\beta$-VAE and the VAE under different settings.
        
    \section{Outline of the paper}
        In Chapter \ref{ch:model_reviews}, we do a brief literature survey of the basic autoencoders up to the VAE and the $\beta$-VAE where we do detailed reviews. In Chapter \ref{ch:methodology}, we describe the aims of the experiments, the data sets we use, the methods of comparison, and the full Neural Network architectures and the hyperparameter settings used for training. In Chapter \ref{ch:results}, we present the results from our experiments which are further discussed in Chapter \ref{ch:discussions}. Finally, we summarise our work in Chapter \ref{ch:conclusion} with some suggestions for future work.